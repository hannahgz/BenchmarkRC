{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ICML_FullBPNetExampleNotebook.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM1VplTZXuvKQtlD8k6wA4r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hannahgz/BenchmarkRCStrategies/blob/master/ICML_FullBPNetExampleNotebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MclFAR1YVJxi",
        "outputId": "f3ce900a-7d56-435c-dc83-ac7fb499c8eb"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "!pip uninstall keras\n",
        "!pip install keras==2.2.4 #Keras v 2.3 has a major bug in the reporting of validation loss for models with multiple outputs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "Uninstalling Keras-2.3.1:\n",
            "  Would remove:\n",
            "    /tensorflow-1.15.2/python3.6/Keras-2.3.1.dist-info/*\n",
            "    /tensorflow-1.15.2/python3.6/docs/*\n",
            "    /tensorflow-1.15.2/python3.6/keras/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled Keras-2.3.1\n",
            "Collecting keras==2.2.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/10/aa32dad071ce52b5502266b5c659451cfd6ffcbf14e6c8c4f16c0ff5aaab/Keras-2.2.4-py2.py3-none-any.whl (312kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 8.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (2.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (3.13)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /tensorflow-1.15.2/python3.6 (from keras==2.2.4) (1.0.8)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.4.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.15.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.1.2)\n",
            "Installing collected packages: keras\n",
            "  Found existing installation: Keras 2.4.3\n",
            "    Uninstalling Keras-2.4.3:\n",
            "      Successfully uninstalled Keras-2.4.3\n",
            "Successfully installed keras-2.2.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USVfscCrW52l",
        "outputId": "b2fa0fc6-5d2f-4871-c2bc-4bda3bd95fac"
      },
      "source": [
        "!apt-get install bedtools\n",
        "!pip install pyfaidx\n",
        "!pip install pyBigWig"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "bedtools is already the newest version (2.26.0+dfsg-5).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 13 not upgraded.\n",
            "Requirement already satisfied: pyfaidx in /usr/local/lib/python3.6/dist-packages (0.5.9.2)\n",
            "Requirement already satisfied: setuptools>=0.7 in /usr/local/lib/python3.6/dist-packages (from pyfaidx) (51.3.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from pyfaidx) (1.15.0)\n",
            "Requirement already satisfied: pyBigWig in /usr/local/lib/python3.6/dist-packages (0.3.17)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWUC6w2IVb0q",
        "outputId": "93e218d9-3ceb-437e-d992-5bddcb3f1fa4"
      },
      "source": [
        "!wget http://hgdownload.cse.ucsc.edu/goldenPath/mm10/bigZips/mm10.chrom.sizes -O mm10.chrom.sizes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-02 16:29:53--  http://hgdownload.cse.ucsc.edu/goldenPath/mm10/bigZips/mm10.chrom.sizes\n",
            "Resolving hgdownload.cse.ucsc.edu (hgdownload.cse.ucsc.edu)... 128.114.119.163\n",
            "Connecting to hgdownload.cse.ucsc.edu (hgdownload.cse.ucsc.edu)|128.114.119.163|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1405 (1.4K)\n",
            "Saving to: ‘mm10.chrom.sizes’\n",
            "\n",
            "\rmm10.chrom.sizes      0%[                    ]       0  --.-KB/s               \rmm10.chrom.sizes    100%[===================>]   1.37K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-02-02 16:29:53 (204 MB/s) - ‘mm10.chrom.sizes’ saved [1405/1405]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oyq_Tyu9XP0d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "926ad463-35c2-4521-d17a-a5e57b793d0f"
      },
      "source": [
        "#Get 1000bp around summit\n",
        "#Example for Sox2, same could be done with Oct4, Nanog, and Klf4\n",
        "!wget http://mitra.stanford.edu/kundaje/avanti/bpnet/trainingdata/bpnet_data/Sox2/idr-optimal-set.summit.bed.gz \n",
        "    \n",
        "!zcat idr-optimal-set.summit.bed.gz | perl -lane 'print $F[0].\"\\t\".(($F[1]+$F[9])).\"\\t\".(($F[1]+$F[9]))' | bedtools slop -g mm10.chrom.sizes -b 500 | perl -lane 'if ($F[2]-$F[1]==1000) {print $F[0].\"\\t\".$F[1].\"\\t\".$F[2].\"\\t1\"}' | sortBed | gzip -c > bpnet_SOX2_1k_around_summits.bed.gz    \n",
        "![[ -f bpnet_SOX2_test_1k_around_summits.bed.gz ]] || zcat bpnet_SOX2_1k_around_summits.bed.gz     | egrep -w 'chr2|chr3|chr4' | gzip -c > bpnet_SOX2_test_1k_around_summits.bed.gz\n",
        "![[ -f bpnet_SOX2_valid_1k_around_summits.bed.gz ]] || zcat bpnet_SOX2_1k_around_summits.bed.gz     | egrep -w 'chr1|chr8|chr9' | gzip -c > bpnet_SOX2_valid_1k_around_summits.bed.gz\n",
        "![[ -f bpnet_SOX2_train_1k_around_summits.bed.gz ]] || zcat bpnet_SOX2_1k_around_summits.bed.gz     | egrep -w -v 'chr1|chr2|chr3|chr4|chr8|chr9' | gzip -c > bpnet_SOX2_train_1k_around_summits.bed.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-02 16:29:53--  http://mitra.stanford.edu/kundaje/avanti/bpnet/trainingdata/bpnet_data/Sox2/idr-optimal-set.summit.bed.gz\n",
            "Resolving mitra.stanford.edu (mitra.stanford.edu)... 171.67.96.243\n",
            "Connecting to mitra.stanford.edu (mitra.stanford.edu)|171.67.96.243|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 89290 (87K) [application/x-gzip]\n",
            "Saving to: ‘idr-optimal-set.summit.bed.gz’\n",
            "\n",
            "\r          idr-optim   0%[                    ]       0  --.-KB/s               \ridr-optimal-set.sum 100%[===================>]  87.20K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2021-02-02 16:29:53 (1.96 MB/s) - ‘idr-optimal-set.summit.bed.gz’ saved [89290/89290]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WkqAQyMXpTD",
        "outputId": "b82af1dd-8504-459e-951c-78f3809e711e"
      },
      "source": [
        "!wget http://mitra.stanford.edu/kundaje/avanti/bpnet/trainingdata/bpnet_data/patchcap/counts.neg.bw\n",
        "!wget http://mitra.stanford.edu/kundaje/avanti/bpnet/trainingdata/bpnet_data/patchcap/counts.pos.bw \n",
        "!wget http://mitra.stanford.edu/kundaje/avanti/bpnet/trainingdata/bpnet_data/mm10_no_alt_analysis_set_ENCODE.fasta\t"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-02 16:29:54--  http://mitra.stanford.edu/kundaje/avanti/bpnet/trainingdata/bpnet_data/patchcap/counts.neg.bw\n",
            "Resolving mitra.stanford.edu (mitra.stanford.edu)... 171.67.96.243\n",
            "Connecting to mitra.stanford.edu (mitra.stanford.edu)|171.67.96.243|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 160921957 (153M)\n",
            "Saving to: ‘counts.neg.bw’\n",
            "\n",
            "counts.neg.bw       100%[===================>] 153.47M  85.0MB/s    in 1.8s    \n",
            "\n",
            "2021-02-02 16:29:55 (85.0 MB/s) - ‘counts.neg.bw’ saved [160921957/160921957]\n",
            "\n",
            "--2021-02-02 16:29:56--  http://mitra.stanford.edu/kundaje/avanti/bpnet/trainingdata/bpnet_data/patchcap/counts.pos.bw\n",
            "Resolving mitra.stanford.edu (mitra.stanford.edu)... 171.67.96.243\n",
            "Connecting to mitra.stanford.edu (mitra.stanford.edu)|171.67.96.243|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 160925907 (153M)\n",
            "Saving to: ‘counts.pos.bw’\n",
            "\n",
            "counts.pos.bw       100%[===================>] 153.47M  84.0MB/s    in 1.8s    \n",
            "\n",
            "2021-02-02 16:29:57 (84.0 MB/s) - ‘counts.pos.bw’ saved [160925907/160925907]\n",
            "\n",
            "--2021-02-02 16:29:58--  http://mitra.stanford.edu/kundaje/avanti/bpnet/trainingdata/bpnet_data/mm10_no_alt_analysis_set_ENCODE.fasta\n",
            "Resolving mitra.stanford.edu (mitra.stanford.edu)... 171.67.96.243\n",
            "Connecting to mitra.stanford.edu (mitra.stanford.edu)|171.67.96.243|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2785490220 (2.6G)\n",
            "Saving to: ‘mm10_no_alt_analysis_set_ENCODE.fasta’\n",
            "\n",
            "mm10_no_alt_analysi 100%[===================>]   2.59G  92.7MB/s    in 29s     \n",
            "\n",
            "2021-02-02 16:30:27 (90.6 MB/s) - ‘mm10_no_alt_analysis_set_ENCODE.fasta’ saved [2785490220/2785490220]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVdfGiEwbmMs",
        "outputId": "27f0f08e-886f-4f50-dfcc-e602bd965280"
      },
      "source": [
        "![[ -e seqdataloader ]] && rm -rf seqdataloader\n",
        "!git clone https://github.com/kundajelab/seqdataloader.git\n",
        "%cd seqdataloader\n",
        "!pip uninstall seqdataloader\n",
        "!pip install .\n",
        "%cd .."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'seqdataloader'...\n",
            "remote: Enumerating objects: 1675, done.\u001b[K\n",
            "remote: Total 1675 (delta 0), reused 0 (delta 0), pack-reused 1675\u001b[K\n",
            "Receiving objects: 100% (1675/1675), 4.02 MiB | 37.04 MiB/s, done.\n",
            "Resolving deltas: 100% (1043/1043), done.\n",
            "/content/seqdataloader\n",
            "\u001b[33mWARNING: Skipping seqdataloader as it is not installed.\u001b[0m\n",
            "Processing /content/seqdataloader\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.6/dist-packages (from seqdataloader==1.2) (1.19.5)\n",
            "Requirement already satisfied: pandas>=0.23.4 in /usr/local/lib/python3.6/dist-packages (from seqdataloader==1.2) (1.1.5)\n",
            "Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.6/dist-packages (from seqdataloader==1.2) (0.29.21)\n",
            "Collecting deeptools>=3.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/f3/789edda975fcca4736fab2007d82cab2e86739901c88bb0528db5c338d1f/deepTools-3.5.0.tar.gz (199kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 8.0MB/s \n",
            "\u001b[?25hCollecting pybedtools>=0.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bf/14/152220f39cda6b9b72810eeed103c6ec78422429adabe3aafc3eaf6feb40/pybedtools-0.8.1.tar.gz (12.5MB)\n",
            "\u001b[K     |████████████████████████████████| 12.5MB 356kB/s \n",
            "\u001b[?25hRequirement already satisfied: pyBigWig>=0.3.7 in /usr/local/lib/python3.6/dist-packages (from seqdataloader==1.2) (0.3.17)\n",
            "Requirement already satisfied: pyfaidx in /usr/local/lib/python3.6/dist-packages (from seqdataloader==1.2) (0.5.9.2)\n",
            "Collecting tiledb>=0.4.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/65/cb/0c945f953150137a68fb73423ae3407ebc63787e562679d10e1e2b59c2a4/tiledb-0.8.1-cp36-cp36m-manylinux2010_x86_64.whl (13.0MB)\n",
            "\u001b[K     |████████████████████████████████| 13.0MB 32.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.23.4->seqdataloader==1.2) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.23.4->seqdataloader==1.2) (2018.9)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from deeptools>=3.0.1->seqdataloader==1.2) (1.4.1)\n",
            "Requirement already satisfied: matplotlib>=3.1.0 in /usr/local/lib/python3.6/dist-packages (from deeptools>=3.0.1->seqdataloader==1.2) (3.2.2)\n",
            "Collecting pysam>=0.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/a1/73e80a7a873f3fb0e52d368a4343eb9882b737c932b95020d82251f1087e/pysam-0.16.0.1-cp36-cp36m-manylinux1_x86_64.whl (9.9MB)\n",
            "\u001b[K     |████████████████████████████████| 10.0MB 43.9MB/s \n",
            "\u001b[?25hCollecting numpydoc>=0.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/1d/9e398c53d6ae27d5ab312ddc16a9ffe1bee0dfdf1d6ec88c40b0ca97582e/numpydoc-1.1.0-py3-none-any.whl (47kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 4.7MB/s \n",
            "\u001b[?25hCollecting py2bit>=0.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/53/bb/547a927bed736ead3dc909e1e552d57c9034bb9493eff80544c0cf6e4828/py2bit-0.3.0.tar.gz\n",
            "Requirement already satisfied: plotly>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from deeptools>=3.0.1->seqdataloader==1.2) (4.4.1)\n",
            "Collecting deeptoolsintervals>=0.1.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/1f/d10d6ad23c86c62d90d867d0506881a392ec6ef06885b858eaab868dd356/deeptoolsintervals-0.1.9.tar.gz (41kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from pybedtools>=0.7->seqdataloader==1.2) (1.15.0)\n",
            "Requirement already satisfied: setuptools>=0.7 in /usr/local/lib/python3.6/dist-packages (from pyfaidx->seqdataloader==1.2) (51.3.3)\n",
            "Requirement already satisfied: wheel>=0.30 in /usr/local/lib/python3.6/dist-packages (from tiledb>=0.4.4->seqdataloader==1.2) (0.36.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.1.0->deeptools>=3.0.1->seqdataloader==1.2) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.1.0->deeptools>=3.0.1->seqdataloader==1.2) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.1.0->deeptools>=3.0.1->seqdataloader==1.2) (0.10.0)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.6/dist-packages (from numpydoc>=0.5->deeptools>=3.0.1->seqdataloader==1.2) (2.11.2)\n",
            "Requirement already satisfied: sphinx>=1.6.5 in /usr/local/lib/python3.6/dist-packages (from numpydoc>=0.5->deeptools>=3.0.1->seqdataloader==1.2) (1.8.5)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly>=2.0.0->deeptools>=3.0.1->seqdataloader==1.2) (1.3.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.3->numpydoc>=0.5->deeptools>=3.0.1->seqdataloader==1.2) (1.1.1)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.5->deeptools>=3.0.1->seqdataloader==1.2) (0.7.12)\n",
            "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.5->deeptools>=3.0.1->seqdataloader==1.2) (1.2.4)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.5->deeptools>=3.0.1->seqdataloader==1.2) (2.6.1)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.5->deeptools>=3.0.1->seqdataloader==1.2) (1.2.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.5->deeptools>=3.0.1->seqdataloader==1.2) (20.8)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.5->deeptools>=3.0.1->seqdataloader==1.2) (2.9.0)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.5->deeptools>=3.0.1->seqdataloader==1.2) (2.1.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.5->deeptools>=3.0.1->seqdataloader==1.2) (2.23.0)\n",
            "Requirement already satisfied: docutils>=0.11 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.5->deeptools>=3.0.1->seqdataloader==1.2) (0.16)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml in /usr/local/lib/python3.6/dist-packages (from sphinxcontrib-websupport->sphinx>=1.6.5->numpydoc>=0.5->deeptools>=3.0.1->seqdataloader==1.2) (1.1.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->sphinx>=1.6.5->numpydoc>=0.5->deeptools>=3.0.1->seqdataloader==1.2) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->sphinx>=1.6.5->numpydoc>=0.5->deeptools>=3.0.1->seqdataloader==1.2) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->sphinx>=1.6.5->numpydoc>=0.5->deeptools>=3.0.1->seqdataloader==1.2) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->sphinx>=1.6.5->numpydoc>=0.5->deeptools>=3.0.1->seqdataloader==1.2) (2.10)\n",
            "Building wheels for collected packages: seqdataloader, deeptools, pybedtools, py2bit, deeptoolsintervals\n",
            "  Building wheel for seqdataloader (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqdataloader: filename=seqdataloader-1.2-cp36-none-any.whl size=38616 sha256=af45c578c99ae7bd3484d9ac50f07e0f1812de2acf3dbe89c82d8edd595a8cd2\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-aj5sqkqm/wheels/c2/db/13/112d41662f69fb8c7986c218293570cc1550fc21eed966e31b\n",
            "  Building wheel for deeptools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deeptools: filename=deepTools-3.5.0-cp36-none-any.whl size=217963 sha256=0c7ca68f81b773e2bc78fdfee42dba426eeb19bd7f1efe86124864308baa556f\n",
            "  Stored in directory: /root/.cache/pip/wheels/15/9b/9c/795a2df551adc3e3208c8caf4e369e6aaf9af1c6f423ea6c39\n",
            "  Building wheel for pybedtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pybedtools: filename=pybedtools-0.8.1-cp36-cp36m-linux_x86_64.whl size=13603779 sha256=9443fd312a8b53f3878a37d25fb097f502cedca1ba3001dcdf9748d5ceb363c6\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/50/97/7d0e4f605d0d1578997f4bba3061869c2dee9f8cd29f626323\n",
            "  Building wheel for py2bit (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py2bit: filename=py2bit-0.3.0-cp36-cp36m-linux_x86_64.whl size=43533 sha256=7acaa3033e1509f6b9d5c9a16fe3ff23754928f75634cc423f52336279b50b0d\n",
            "  Stored in directory: /root/.cache/pip/wheels/df/66/b6/33fb9b65b31121127f1da60ca27948ecf8d4c59b0967298de8\n",
            "  Building wheel for deeptoolsintervals (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deeptoolsintervals: filename=deeptoolsintervals-0.1.9-cp36-cp36m-linux_x86_64.whl size=108529 sha256=23c3c3c5f4a93d4610a106fda988eee7af7dad9ad261597c4eb6def1f1ff2bf7\n",
            "  Stored in directory: /root/.cache/pip/wheels/f1/60/60/e513c6246f67379f6e1b8d09448cdf913bac3851f96bd42e94\n",
            "Successfully built seqdataloader deeptools pybedtools py2bit deeptoolsintervals\n",
            "Installing collected packages: pysam, numpydoc, py2bit, deeptoolsintervals, deeptools, pybedtools, tiledb, seqdataloader\n",
            "Successfully installed deeptools-3.5.0 deeptoolsintervals-0.1.9 numpydoc-1.1.0 py2bit-0.3.0 pybedtools-0.8.1 pysam-0.16.0.1 seqdataloader-1.2 tiledb-0.8.1\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dH-oBMa-b69T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcb02f77-4fe6-408b-e448-43f216ac0833"
      },
      "source": [
        "import seqdataloader\n",
        "from seqdataloader.batchproducers import coordbased\n",
        "from seqdataloader.batchproducers.coordbased import coordbatchproducers\n",
        "from seqdataloader.batchproducers.coordbased import coordstovals\n",
        "from seqdataloader.batchproducers.coordbased import coordbatchproducers\n",
        "from seqdataloader.batchproducers.coordbased import coordbatchtransformers\n",
        "from seqdataloader.batchproducers.coordbased.coordbatchproducers import SimpleCoordsBatchProducer\n",
        "from seqdataloader.batchproducers.coordbased.coordstovals.bigwig import AbstractCountAndProfileTransformer \n",
        "from seqdataloader.batchproducers.coordbased.coordstovals.bigwig import LogCountsPlusOne\n",
        "from seqdataloader.batchproducers.coordbased.coordstovals.bigwig import SmoothProfiles\n",
        "from seqdataloader.batchproducers.coordbased.coordstovals.bigwig import BigWigReader \n",
        "from seqdataloader.batchproducers.coordbased.coordstovals.bigwig import smooth_profiles\n",
        "from seqdataloader.batchproducers.coordbased.coordstovals.bigwig import rolling_window\n",
        "from seqdataloader.batchproducers.coordbased.coordbatchtransformers import AbstractCoordBatchTransformer\n",
        "from seqdataloader.batchproducers.coordbased.coordbatchtransformers import get_revcomp\n",
        "from seqdataloader.batchproducers.coordbased.core import Coordinates, KerasBatchGenerator, apply_mask"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqbLZilsbyj9",
        "outputId": "1e802540-c6f4-4df6-9a2f-a22e4dfd44a0"
      },
      "source": [
        "!pip install keras-genomics\n",
        "import keras_genomics\n",
        "from keras_genomics.layers.convolutional import RevCompConv1D"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras-genomics in /usr/local/lib/python3.6/dist-packages (0.1.2.1)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from keras-genomics) (2.2.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->keras-genomics) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->keras-genomics) (2.10.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /tensorflow-1.15.2/python3.6 (from keras->keras-genomics) (1.0.8)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras->keras-genomics) (1.19.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras->keras-genomics) (1.15.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras->keras-genomics) (1.1.2)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras->keras-genomics) (1.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqAjQkR0cJv9"
      },
      "source": [
        "import keras\n",
        "from keras import backend as K \n",
        "import keras.layers as kl\n",
        "from keras.engine import Layer\n",
        "from keras.engine.base_layer import InputSpec\n",
        "from keras.callbacks import History\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ue_OpmgeatQ2"
      },
      "source": [
        "def get_inputs_and_targets(dataset, seq_len, out_pred_len): \n",
        "    inputs_coordstovals = coordstovals.core.CoordsToValsJoiner(\n",
        "        coordstovals_list=[\n",
        "            coordbased.coordstovals.fasta.PyfaidxCoordsToVals(\n",
        "            genome_fasta_path=\"mm10_no_alt_analysis_set_ENCODE.fasta\",\n",
        "            mode_name=\"sequence\",\n",
        "            center_size_to_use=seq_len),\n",
        "            coordstovals.bigwig.PosAndNegSmoothWindowCollapsedLogCounts(\n",
        "                pos_strand_bigwig_path=\"counts.pos.bw\",\n",
        "                neg_strand_bigwig_path=\"counts.neg.bw\",\n",
        "                counts_mode_name=\"patchcap.logcount\",\n",
        "                profile_mode_name=\"patchcap.profile\",\n",
        "                center_size_to_use=out_pred_len,\n",
        "                smoothing_windows=[1,50])])\n",
        "    \n",
        "    targets_coordstovals = coordstovals.bigwig.PosAndNegSeparateLogCounts(\n",
        "        counts_mode_name=\"CHIPNexus.%s.logcount\" % dataset,\n",
        "        profile_mode_name=\"CHIPNexus.%s.profile\" % dataset,\n",
        "        pos_strand_bigwig_path=\"counts.pos.bw\",\n",
        "        neg_strand_bigwig_path=\"counts.neg.bw\",\n",
        "        center_size_to_use=out_pred_len)\n",
        "   \n",
        "    return inputs_coordstovals, targets_coordstovals\n",
        "\n",
        "\n",
        "def get_train_generator(PARAMETERS, inputs_coordstovals, targets_coordstovals, model_arch): \n",
        "    train_file = \"bpnet_SOX2_train_1k_around_summits.bed.gz\" \n",
        "    chromsizes_file=\"mm10.chrom.sizes\"\n",
        "    \n",
        "    if model_arch == \"Standard-RCAug\":\n",
        "        train_batch_generator = KerasBatchGenerator(\n",
        "            coordsbatch_producer=coordbatchproducers.SimpleCoordsBatchProducer(\n",
        "                bed_file=train_file,\n",
        "                batch_size=64,\n",
        "                shuffle_before_epoch=True, \n",
        "                seed=PARAMETERS['seed']),\n",
        "            inputs_coordstovals=inputs_coordstovals,\n",
        "            targets_coordstovals=targets_coordstovals,\n",
        "            coordsbatch_transformer=coordbatchtransformers.ReverseComplementAugmenter().chain(\n",
        "                coordbatchtransformers.UniformJitter(\n",
        "                    maxshift=200, chromsizes_file=chromsizes_file)))\n",
        "    else:\n",
        "        train_batch_generator = KerasBatchGenerator(\n",
        "            coordsbatch_producer=coordbatchproducers.SimpleCoordsBatchProducer(\n",
        "                bed_file = train_file,\n",
        "                batch_size=64,\n",
        "                shuffle_before_epoch=True, \n",
        "                seed=PARAMETERS['seed']),\n",
        "            inputs_coordstovals=inputs_coordstovals,\n",
        "            targets_coordstovals=targets_coordstovals,\n",
        "            coordsbatch_transformer=coordbatchtransformers.UniformJitter(\n",
        "                maxshift=200, chromsizes_file=chromsizes_file))\n",
        "        \n",
        "    return train_batch_generator\n",
        "\n",
        "\n",
        "def get_val_generator(PARAMETERS, inputs_coordstovals, targets_coordstovals): \n",
        "    valid_file = \"bpnet_SOX2_valid_1k_around_summits.bed.gz\"\n",
        "    \n",
        "    val_batch_generator = KerasBatchGenerator(\n",
        "      coordsbatch_producer=coordbatchproducers.SimpleCoordsBatchProducer(\n",
        "                bed_file = valid_file,\n",
        "                batch_size=64,\n",
        "                shuffle_before_epoch=False, \n",
        "                seed=PARAMETERS['seed']),\n",
        "      inputs_coordstovals=inputs_coordstovals,\n",
        "      targets_coordstovals=targets_coordstovals)\n",
        "    \n",
        "    return val_batch_generator\n",
        "        \n",
        "\n",
        "class GeneralReverseComplement(AbstractCoordBatchTransformer):\n",
        "    def __call__(self, coords):\n",
        "        return [get_revcomp(x) for x in coords]\n",
        "    \n",
        "    \n",
        "def get_test_generator(PARAMETERS, inputs_coordstovals, targets_coordstovals):\n",
        "    test_file = \"bpnet_SOX2_test_1k_around_summits.bed.gz\"\n",
        "    chromsizes_file=\"mm10.chrom.sizes\"\n",
        "        \n",
        "    keras_test_batch_generator = KerasBatchGenerator(\n",
        "      coordsbatch_producer=coordbatchproducers.SimpleCoordsBatchProducer(\n",
        "                bed_file = test_file,\n",
        "                batch_size=64,\n",
        "                shuffle_before_epoch=False, \n",
        "                seed=PARAMETERS['seed']),\n",
        "      inputs_coordstovals=inputs_coordstovals,\n",
        "      targets_coordstovals=targets_coordstovals)\n",
        "    \n",
        "    keras_rc_test_batch_generator  = KerasBatchGenerator(\n",
        "      coordsbatch_producer=coordbatchproducers.SimpleCoordsBatchProducer(\n",
        "          bed_file=test_file,\n",
        "          batch_size=64,\n",
        "          shuffle_before_epoch=False, \n",
        "          seed=PARAMETERS['seed']),\n",
        "      inputs_coordstovals=inputs_coordstovals,\n",
        "      targets_coordstovals=targets_coordstovals,\n",
        "      coordsbatch_transformer=GeneralReverseComplement())\n",
        "    \n",
        "    return keras_test_batch_generator, keras_rc_test_batch_generator\n",
        "    \n",
        "    \n",
        "def save_results(PARAMETERS, model, model_arch, model_history):    \n",
        "    txt_file_name = (\"%s.txt\" % (model_arch))\n",
        "        \n",
        "    loss_file = open(txt_file_name, \"w\")\n",
        "    loss_file.write(\"model parameters\" + \"\\n\")\n",
        "    for x in PARAMETERS: \n",
        "        loss_file.write(str(x) + \": \" + str(PARAMETERS[x]) + \"\\n\")\n",
        "    \n",
        "    loss_file.write(\"val_loss\\n\")\n",
        "    for row in model_history.history[\"val_loss\"]: \n",
        "        loss_file.write(str(row) + \"\\n\")\n",
        "    loss_file.write(\"min val loss: \" + str(np.min(model_history.history[\"val_loss\"])))  \n",
        "    \n",
        "    loss_file.close()\n",
        "    if PARAMETERS['filters'] == 32:\n",
        "        model_save_name = (\"%s-half.h5\" % (model_arch))    \n",
        "    else: \n",
        "        model_save_name = (\"%s.h5\" % (model_arch)) \n",
        "        \n",
        "    model.save(model_save_name)\n",
        "    \n",
        "\n",
        "def train_model(PARAMETERS, inputs_coordstovals, targets_coordstovals, epochs_to_train_for, model, model_arch): \n",
        "    train_batch_generator = get_train_generator(PARAMETERS, inputs_coordstovals, targets_coordstovals, model_arch)\n",
        "    val_batch_generator = get_val_generator(PARAMETERS, inputs_coordstovals, targets_coordstovals)\n",
        " \n",
        "    early_stopping_callback = keras.callbacks.EarlyStopping(\n",
        "                              patience=10, restore_best_weights=True)\n",
        "    \n",
        "    model_history = History()\n",
        "    model.fit_generator(train_batch_generator,\n",
        "                        epochs = epochs_to_train_for, \n",
        "                        validation_data=val_batch_generator,\n",
        "                        callbacks=[early_stopping_callback, model_history])\n",
        "    model.set_weights(early_stopping_callback.best_weights)\n",
        "    save_results(PARAMETERS, model, model_arch, model_history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZN8TEh09bIfK"
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "from keras.layers.core import Dropout \n",
        "from keras import backend as K \n",
        "from keras.engine import Layer\n",
        "from keras.engine.base_layer import InputSpec\n",
        "from keras.callbacks import History\n",
        "from keras.optimizers import Optimizer\n",
        "\n",
        "from tensorflow.python.eager import def_function\n",
        "from tensorflow.python.framework import ops\n",
        "from tensorflow.python.keras import backend_config\n",
        "from tensorflow.python.keras.optimizer_v2 import optimizer_v2\n",
        "from tensorflow.python.ops import array_ops\n",
        "from tensorflow.python.ops import control_flow_ops\n",
        "from tensorflow.python.ops import math_ops\n",
        "from tensorflow.python.ops import state_ops\n",
        "from tensorflow.python.training import training_ops\n",
        "from tensorflow.python.util.tf_export import keras_export\n",
        "\n",
        "#Loss Function\n",
        "def multinomial_nll(true_counts, logits):\n",
        "    \"\"\"Compute the multinomial negative log-likelihood\n",
        "    Args:\n",
        "      true_counts: observed count values\n",
        "      logits: predicted logit values\n",
        "    \"\"\"\n",
        "    counts_per_example = tf.reduce_sum(true_counts, axis=-1)\n",
        "    dist = tf.compat.v1.distributions.Multinomial(total_count=counts_per_example,\n",
        "                                         logits=logits)\n",
        "    return (-tf.reduce_sum(dist.log_prob(true_counts)) / \n",
        "            tf.cast((tf.shape(true_counts)[0]), tf.float32))\n",
        "\n",
        "\n",
        "#from https://github.com/kundajelab/basepair/blob/cda0875571066343cdf90aed031f7c51714d991a/basepair/losses.py#L87\n",
        "class MultichannelMultinomialNLL(object):\n",
        "    def __init__(self, n):\n",
        "        self.__name__ = \"MultichannelMultinomialNLL\"\n",
        "        self.n = n\n",
        "\n",
        "    def __call__(self, true_counts, logits):\n",
        "        for i in range(self.n):\n",
        "            loss = multinomial_nll(true_counts[..., i], logits[..., i])\n",
        "            if i == 0:\n",
        "                total = loss\n",
        "            else:\n",
        "                total += loss\n",
        "        return total\n",
        "\n",
        "    def get_config(self):\n",
        "        return {\"n\": self.n}\n",
        "\n",
        "    \n",
        "class AbstractProfileModel(object):\n",
        "    def __init__(self, dataset, input_seq_len, c_task_weight, p_task_weight, \n",
        "                 filters, n_dil_layers, conv1_kernel_size, dil_kernel_size,\n",
        "                 outconv_kernel_size, optimizer, weight_decay, lr, \n",
        "                 kernel_initializer, seed):\n",
        "        self.dataset = dataset\n",
        "        self.input_seq_len = input_seq_len\n",
        "        self.c_task_weight = c_task_weight\n",
        "        self.p_task_weight = p_task_weight\n",
        "        self.filters = filters\n",
        "        self.n_dil_layers = n_dil_layers\n",
        "        self.conv1_kernel_size = conv1_kernel_size\n",
        "        self.dil_kernel_size = dil_kernel_size\n",
        "        self.outconv_kernel_size = outconv_kernel_size\n",
        "        self.optimizer = optimizer\n",
        "        self.weight_decay = weight_decay\n",
        "        self.lr = lr\n",
        "        self.learning_rate = lr\n",
        "        self.kernel_initializer = kernel_initializer\n",
        "        self.seed = seed\n",
        "    \n",
        "    def get_embedding_len(self):\n",
        "        embedding_len = self.input_seq_len\n",
        "        embedding_len -= (self.conv1_kernel_size-1)     \n",
        "        for i in range(1, self.n_dil_layers+1):\n",
        "            dilation_rate = (2**i)\n",
        "            embedding_len -= dilation_rate*(self.dil_kernel_size-1)\n",
        "        return embedding_len\n",
        "    \n",
        "    def get_output_profile_len(self):\n",
        "        embedding_len = self.get_embedding_len()\n",
        "        out_profile_len = embedding_len - (self.outconv_kernel_size - 1)\n",
        "        return out_profile_len\n",
        "    \n",
        "    def trim_flanks_of_conv_layer(self, conv_layer, output_len, width_to_trim, filters):\n",
        "        layer = keras.layers.Lambda(\n",
        "            lambda x: x[:,\n",
        "              int(0.5*(width_to_trim)):-(width_to_trim-int(0.5*(width_to_trim)))],\n",
        "            output_shape=(output_len, filters))(conv_layer)\n",
        "        return layer \n",
        "    \n",
        "    \n",
        "    def get_inputs(self):\n",
        "        out_pred_len = self.get_output_profile_len()\n",
        "        \n",
        "        inp = kl.Input(shape=(self.input_seq_len, 4), name='sequence')\n",
        "        bias_counts_input = kl.Input(shape=(1,), name=\"patchcap.logcount\")\n",
        "        #if working with raw counts, go from logcount->count\n",
        "        bias_profile_input = kl.Input(shape=(out_pred_len, 2),\n",
        "                                    name=\"patchcap.profile\")\n",
        "        return inp, bias_counts_input, bias_profile_input\n",
        "    \n",
        "    def get_names(self): \n",
        "        countouttaskname = \"CHIPNexus.%s.logcount\" % self.dataset\n",
        "        profileouttaskname = \"CHIPNexus.%s.profile\" % self.dataset\n",
        "        return countouttaskname, profileouttaskname\n",
        "            \n",
        "    def get_keras_model(self): \n",
        "        raise NotImplementedError()\n",
        "\n",
        "        \n",
        "class RCBPNetArch(AbstractProfileModel):   \n",
        "    def __init__(self, is_add, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.is_add = is_add\n",
        "        \n",
        "    def get_keras_model(self):\n",
        "        np.random.seed(self.seed)\n",
        "        tf.set_random_seed(self.seed)\n",
        "        \n",
        "        inp, bias_counts_input, bias_profile_input = self.get_inputs()\n",
        "        countouttaskname, profileouttaskname = self.get_names()\n",
        "        \n",
        "        out_pred_len = self.get_output_profile_len()\n",
        "        curr_layer_size = self.input_seq_len - (self.conv1_kernel_size-1)\n",
        "        \n",
        "        first_conv = RevCompConv1D(filters=self.filters,\n",
        "                                   kernel_size=self.conv1_kernel_size,\n",
        "                                   kernel_initializer = self.kernel_initializer,\n",
        "                                   padding='valid',\n",
        "                                   activation='relu')(inp)\n",
        "\n",
        "        prev_layers = first_conv\n",
        "        for i in range(1, self.n_dil_layers + 1):\n",
        "            dilation_rate = 2**i\n",
        "    \n",
        "            conv_output = RevCompConv1D(filters=self.filters,\n",
        "                                        kernel_size=self.dil_kernel_size,\n",
        "                                        kernel_initializer = self.kernel_initializer,\n",
        "                                        padding='valid',\n",
        "                                        activation='relu',\n",
        "                                        dilation_rate=dilation_rate)(prev_layers)   \n",
        "\n",
        "            width_to_trim = dilation_rate*(self.dil_kernel_size-1)\n",
        "\n",
        "            curr_layer_size = (curr_layer_size - width_to_trim)\n",
        "            \n",
        "\n",
        "            prev_layers = self.trim_flanks_of_conv_layer(\n",
        "                conv_layer = prev_layers, output_len = curr_layer_size, \n",
        "                width_to_trim = width_to_trim, filters = 2 * self.filters)\n",
        "            \n",
        "            if(self.is_add): \n",
        "                prev_layers = kl.add([prev_layers, conv_output])\n",
        "            else:\n",
        "                prev_layers = kl.average([prev_layers, conv_output])\n",
        "\n",
        "        combined_conv = prev_layers\n",
        "\n",
        "        #Counts prediction\n",
        "        gap_combined_conv = kl.GlobalAvgPool1D()(combined_conv)\n",
        "        count_out = kl.Reshape((-1,), name=countouttaskname)(\n",
        "            RevCompConv1D(filters=1, kernel_size=1, kernel_initializer = self.kernel_initializer)(\n",
        "              kl.Reshape((1,-1))(kl.concatenate([\n",
        "                  #concatenation of the bias layer both before and after\n",
        "                  # is needed for rc symmetry\n",
        "                  kl.Lambda(lambda x: x[:, ::-1])(bias_counts_input),\n",
        "                  gap_combined_conv,\n",
        "                  bias_counts_input], axis=-1))))\n",
        "\n",
        "        #Profile prediction\n",
        "        profile_out_prebias = RevCompConv1D(\n",
        "            filters=1,kernel_size=self.outconv_kernel_size,\n",
        "            kernel_initializer = self.kernel_initializer, padding='valid')(combined_conv)\n",
        "        \n",
        "        profile_out = RevCompConv1D(\n",
        "            filters=1, kernel_size=1, name=profileouttaskname, kernel_initializer = self.kernel_initializer)(\n",
        "                    kl.concatenate([\n",
        "                        #concatenation of the bias layer both before and after\n",
        "                        # is needed for rc symmetry\n",
        "                        kl.Lambda(lambda x: x[:, :, ::-1])(bias_profile_input),\n",
        "                        profile_out_prebias,\n",
        "                        bias_profile_input], axis=-1))\n",
        "            \n",
        "        model = keras.models.Model(\n",
        "          inputs=[inp, bias_counts_input, bias_profile_input],\n",
        "          outputs=[count_out, profile_out])\n",
        "        \n",
        "        model.compile(keras.optimizers.Adam(lr=self.lr),\n",
        "                      loss=['mse', MultichannelMultinomialNLL(2)],\n",
        "                      loss_weights=[self.c_task_weight, self.p_task_weight]) \n",
        "          \n",
        "        return model\n",
        "    \n",
        "    \n",
        "class CJTrainedBPNetArch(AbstractProfileModel):\n",
        "    def __init__(self, is_add, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.is_add = is_add\n",
        "    \n",
        "    def trim_flanks_of_conv_layer_revcomp(self, conv_layer, output_len, width_to_trim, filters):\n",
        "        layer = keras.layers.Lambda(\n",
        "            lambda x: x[:,\n",
        "              (width_to_trim-int(0.5*(width_to_trim))):-int(0.5*(width_to_trim))],\n",
        "            output_shape=(output_len, filters))(conv_layer)\n",
        "        return layer \n",
        "        \n",
        "    def get_keras_model(self):\n",
        "        np.random.seed(self.seed)\n",
        "        tf.set_random_seed(self.seed)\n",
        "        \n",
        "        inp, bias_counts_input, bias_profile_input = self.get_inputs()\n",
        "        rev_inp = kl.Lambda(lambda x: x[:,::-1,::-1])(inp)\n",
        "        \n",
        "        countouttaskname, profileouttaskname = self.get_names()\n",
        "        \n",
        "        first_conv = kl.Conv1D(self.filters,\n",
        "                               kernel_size=self.conv1_kernel_size,\n",
        "                               kernel_initializer = self.kernel_initializer,\n",
        "                               padding='valid',\n",
        "                               activation='relu')\n",
        "        first_conv_fwd = first_conv(inp)\n",
        "        first_conv_rev = first_conv(rev_inp)\n",
        "\n",
        "        curr_layer_size = self.input_seq_len - (self.conv1_kernel_size-1)\n",
        "        \n",
        "        prev_layers_fwd = first_conv_fwd\n",
        "        prev_layers_rev = first_conv_rev\n",
        "\n",
        "        for i in range(1, self.n_dil_layers + 1):\n",
        "            dilation_rate = 2**i\n",
        "            conv_output = kl.Conv1D(self.filters, kernel_size=self.dil_kernel_size, \n",
        "                                    padding='valid',\n",
        "                                    kernel_initializer = self.kernel_initializer,\n",
        "                                    activation='relu', \n",
        "                                    dilation_rate=dilation_rate)\n",
        "\n",
        "            conv_output_fwd = conv_output(prev_layers_fwd)\n",
        "            conv_output_rev = conv_output(prev_layers_rev)\n",
        "            \n",
        "            width_to_trim = dilation_rate * (self.dil_kernel_size - 1)\n",
        "            \n",
        "            curr_layer_size = (curr_layer_size - width_to_trim)\n",
        "            \n",
        "            prev_layers_fwd = self.trim_flanks_of_conv_layer(\n",
        "                conv_layer = prev_layers_fwd, output_len = curr_layer_size, \n",
        "                width_to_trim = width_to_trim, filters = self.filters)\n",
        "\n",
        "            prev_layers_rev = self.trim_flanks_of_conv_layer_revcomp(\n",
        "                conv_layer = prev_layers_rev, output_len = curr_layer_size, \n",
        "                width_to_trim = width_to_trim, filters = self.filters)\n",
        "            \n",
        "            if(self.is_add):\n",
        "                prev_layers_fwd = kl.add([prev_layers_fwd, conv_output_fwd])\n",
        "                prev_layers_rev = kl.add([prev_layers_rev, conv_output_rev])\n",
        "            else: \n",
        "                prev_layers_fwd = kl.average([prev_layers_fwd, conv_output_fwd])\n",
        "                prev_layers_rev = kl.average([prev_layers_rev, conv_output_rev])\n",
        "                \n",
        "            combined_conv_fwd = prev_layers_fwd\n",
        "            combined_conv_rev = prev_layers_rev\n",
        "\n",
        "        #Counts Prediction\n",
        "        counts_dense_layer = kl.Dense(2,kernel_initializer = self.kernel_initializer,)\n",
        "        gap_combined_conv_fwd = kl.GlobalAvgPool1D()(combined_conv_fwd)\n",
        "        gap_combined_conv_rev = kl.GlobalAvgPool1D()(combined_conv_rev)\n",
        "        \n",
        "        main_count_out_fwd = counts_dense_layer(\n",
        "            kl.concatenate([gap_combined_conv_fwd, bias_counts_input], axis=-1))\n",
        "        \n",
        "        main_count_out_rev = counts_dense_layer(\n",
        "            kl.concatenate([bias_counts_input, gap_combined_conv_rev], axis=-1))\n",
        "        rc_rev_count_out = kl.Lambda(lambda x: x[:,::-1])(main_count_out_rev)\n",
        "        \n",
        "        avg_count_out = kl.Average(name = countouttaskname)(\n",
        "            [main_count_out_fwd, rc_rev_count_out])\n",
        "\n",
        "        #Profile Prediction\n",
        "        profile_penultimate_conv = kl.Conv1D(filters = 2, \n",
        "                                             kernel_size = self.outconv_kernel_size,\n",
        "                                             kernel_initializer = self.kernel_initializer,\n",
        "                                             padding = 'valid')\n",
        "        profile_final_conv = kl.Conv1D(2, kernel_size=1, kernel_initializer = self.kernel_initializer,)\n",
        "        \n",
        "        profile_out_prebias_fwd = profile_penultimate_conv(combined_conv_fwd)\n",
        "        main_profile_out_fwd = profile_final_conv(kl.concatenate(\n",
        "            [profile_out_prebias_fwd, bias_profile_input], axis=-1))\n",
        "\n",
        "        profile_out_prebias_rev = profile_penultimate_conv(combined_conv_rev)\n",
        "        rev_bias_profile_input = kl.Lambda(lambda x: x[:,::-1,:])(bias_profile_input)\n",
        "        main_profile_out_rev = profile_final_conv(kl.concatenate(\n",
        "            [profile_out_prebias_rev, rev_bias_profile_input], axis=-1))\n",
        "        rc_rev_profile_out = kl.Lambda(lambda x: x[:,::-1,::-1])(main_profile_out_rev)\n",
        "\n",
        "        avg_profile_out = kl.Average(name = profileouttaskname)(\n",
        "            [main_profile_out_fwd, rc_rev_profile_out])    \n",
        "        \n",
        "        model = keras.models.Model(\n",
        "          inputs=[inp, bias_counts_input, bias_profile_input],\n",
        "          outputs=[avg_count_out, avg_profile_out])\n",
        "        \n",
        "        model.compile(keras.optimizers.Adam(lr=self.lr),\n",
        "                      loss=['mse', MultichannelMultinomialNLL(2)],\n",
        "                      loss_weights=[self.c_task_weight, self.p_task_weight]) \n",
        "     \n",
        "        return model\n",
        "    \n",
        "    \n",
        "class CJRCWrapperArch(AbstractProfileModel): \n",
        "    def __init__(self,  **kwargs):\n",
        "        super().__init__(**kwargs) \n",
        "        \n",
        "    def trim_flanks_of_conv_layer_revcomp(self, conv_layer, output_len, width_to_trim, filters):\n",
        "        layer = keras.layers.Lambda(\n",
        "            lambda x: x[:,\n",
        "              (width_to_trim-int(0.5*(width_to_trim))):-int(0.5*(width_to_trim))],\n",
        "            output_shape=(output_len, filters))(conv_layer)\n",
        "        return layer \n",
        "        \n",
        "    def apply_conjoined_rcps(self, curr_submodel, input_tensor):\n",
        "        RC_LAYER = keras.layers.Lambda(lambda x: x[:, ::-1, ::-1])\n",
        "        \n",
        "        #run on the original input tensor\n",
        "        fwd_out = curr_submodel(input_tensor)\n",
        "        #take revcomp of the input tensor\n",
        "        rc_input_tensor = RC_LAYER(input_tensor)\n",
        "        #run on the revcomp of the input tensor\n",
        "        rev_out = curr_submodel(rc_input_tensor)\n",
        "        #reverse the revcomp\n",
        "        rc_rev_out = RC_LAYER(rev_out)\n",
        "        #concatenate rc_rev_out with fwd_out along the\n",
        "        # channel axis, achieving rc equivariance\n",
        "        final_out = keras.layers.Concatenate(axis=2)([fwd_out, rc_rev_out])\n",
        "        return final_out\n",
        "    \n",
        "    def get_keras_model(self):\n",
        "        np.random.seed(self.seed)\n",
        "        tf.set_random_seed(self.seed)\n",
        "        \n",
        "        inp, bias_counts_input, bias_profile_input = self.get_inputs()\n",
        "        countouttaskname, profileouttaskname = self.get_names()\n",
        "        #define the submodel\n",
        "        submodel_inp = keras.layers.Input(shape=(self.input_seq_len, 4))\n",
        "        first_conv = kl.Conv1D(self.filters,\n",
        "                               kernel_size=self.conv1_kernel_size,\n",
        "                               kernel_initializer = self.kernel_initializer,\n",
        "                               padding='valid',\n",
        "                               activation='relu')\n",
        "        first_conv_fwd = first_conv(submodel_inp)\n",
        "        curr_layer_size = self.input_seq_len - (self.conv1_kernel_size-1)\n",
        "        prev_layers_fwd = first_conv_fwd\n",
        "\n",
        "        for i in range(1, self.n_dil_layers + 1):\n",
        "            dilation_rate = 2**i\n",
        "            conv_output = kl.Conv1D(self.filters, kernel_size=self.dil_kernel_size, \n",
        "                                    padding='valid',\n",
        "                                    kernel_initializer = self.kernel_initializer,\n",
        "                                    activation='relu', \n",
        "                                    dilation_rate=dilation_rate)\n",
        "\n",
        "            conv_output_fwd = conv_output(prev_layers_fwd)\n",
        "            \n",
        "            width_to_trim = dilation_rate * (self.dil_kernel_size - 1)\n",
        "            \n",
        "            curr_layer_size = (curr_layer_size - width_to_trim)\n",
        "            \n",
        "            prev_layers_fwd = self.trim_flanks_of_conv_layer(\n",
        "                conv_layer = prev_layers_fwd, output_len = curr_layer_size, \n",
        "                width_to_trim = width_to_trim, filters = self.filters)\n",
        "            \n",
        "            prev_layers_fwd = kl.add([prev_layers_fwd, conv_output_fwd])\n",
        "                \n",
        "        combined_conv_submodel = prev_layers_fwd\n",
        "        submodel = keras.models.Model(inputs = submodel_inp, outputs = combined_conv_submodel)\n",
        "        combined_conv = self.apply_conjoined_rcps(curr_submodel = submodel, input_tensor = inp)\n",
        "        \n",
        "        #Counts prediction\n",
        "        gap_combined_conv = kl.GlobalAvgPool1D()(combined_conv)\n",
        "        count_out = kl.Reshape((-1,), name=countouttaskname)(\n",
        "                    RevCompConv1D(filters=1, kernel_size=1, kernel_initializer = self.kernel_initializer)(\n",
        "                      kl.Reshape((1,-1))(kl.concatenate([\n",
        "                          #concatenation of the bias layer both before and after\n",
        "                          # is needed for rc symmetry\n",
        "                          kl.Lambda(lambda x: x[:, ::-1])(bias_counts_input),\n",
        "                          gap_combined_conv,\n",
        "                          bias_counts_input], axis=-1))))\n",
        "        #Profile prediction\n",
        "        profile_out_prebias = RevCompConv1D(\n",
        "                    filters=1,kernel_size=self.outconv_kernel_size,\n",
        "                    kernel_initializer = self.kernel_initializer, padding='valid')(combined_conv)\n",
        "        profile_out = RevCompConv1D(\n",
        "                    filters=1, kernel_size=1, name=profileouttaskname, kernel_initializer = self.kernel_initializer)(\n",
        "                            kl.concatenate([\n",
        "                                #concatenation of the bias layer both before and after\n",
        "                                # is needed for rc symmetry\n",
        "                                kl.Lambda(lambda x: x[:, :, ::-1])(bias_profile_input),\n",
        "                                profile_out_prebias,\n",
        "                                bias_profile_input], axis=-1))\n",
        "        model = keras.models.Model(\n",
        "                  inputs=[inp, bias_counts_input, bias_profile_input],\n",
        "                  outputs=[count_out, profile_out])\n",
        "        \n",
        "        model.compile(keras.optimizers.Adam(lr=self.lr),\n",
        "                          loss=['mse', MultichannelMultinomialNLL(2)],\n",
        "                          loss_weights=[self.c_task_weight, self.p_task_weight]) \n",
        "        \n",
        "        return model \n",
        "    \n",
        "    \n",
        "class StandardBPNetArch(AbstractProfileModel): \n",
        "    def __init__(self, is_add, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.is_add = is_add\n",
        "        \n",
        "    def get_keras_model(self):\n",
        "        np.random.seed(self.seed)\n",
        "        tf.set_random_seed(self.seed)\n",
        "        \n",
        "        inp, bias_counts_input, bias_profile_input = self.get_inputs()\n",
        "        countouttaskname, profileouttaskname = self.get_names()\n",
        "        \n",
        "        first_conv = kl.Conv1D(self.filters,\n",
        "                               kernel_size=self.conv1_kernel_size,\n",
        "                               kernel_initializer = self.kernel_initializer,\n",
        "                               padding='valid',\n",
        "                               activation='relu')(inp)\n",
        "        curr_layer_size = self.input_seq_len - (self.conv1_kernel_size-1)\n",
        "\n",
        "        prev_layers = first_conv\n",
        "        for i in range(1, self.n_dil_layers + 1):\n",
        "            dilation_rate = 2**i\n",
        "            conv_output = kl.Conv1D(self.filters, kernel_size=self.dil_kernel_size, \n",
        "                                    kernel_initializer = self.kernel_initializer,\n",
        "                                    padding='valid',\n",
        "                                    activation='relu', \n",
        "                                    dilation_rate=dilation_rate)(prev_layers)\n",
        "\n",
        "            width_to_trim = dilation_rate * (self.dil_kernel_size - 1)\n",
        "\n",
        "\n",
        "            curr_layer_size = (curr_layer_size - width_to_trim)\n",
        "            prev_layers = self.trim_flanks_of_conv_layer(\n",
        "              conv_layer = prev_layers, output_len = curr_layer_size, \n",
        "              width_to_trim = width_to_trim, filters = self.filters)\n",
        "\n",
        "            if(self.is_add): \n",
        "                prev_layers = kl.add([prev_layers, conv_output])\n",
        "            else:\n",
        "                prev_layers = kl.average([prev_layers, conv_output])\n",
        "\n",
        "        combined_conv = prev_layers\n",
        "\n",
        "        #Counts Prediction\n",
        "        gap_combined_conv = kl.GlobalAvgPool1D()(combined_conv)\n",
        "        count_out = kl.Dense(2, kernel_initializer = self.kernel_initializer, name=countouttaskname)(\n",
        "            kl.concatenate([gap_combined_conv, bias_counts_input], axis=-1))\n",
        "\n",
        "        #Profile Prediction\n",
        "        profile_out_prebias = kl.Conv1D(filters = 2, \n",
        "                                        kernel_size = self.outconv_kernel_size,\n",
        "                                        kernel_initializer = self.kernel_initializer,\n",
        "                                        padding = 'valid')(combined_conv)\n",
        "        profile_out = kl.Conv1D(2, kernel_size=1, kernel_initializer = self.kernel_initializer, name=profileouttaskname)(\n",
        "            kl.concatenate([profile_out_prebias, bias_profile_input], axis=-1))\n",
        "\n",
        "        model = keras.models.Model(\n",
        "          inputs=[inp, bias_counts_input, bias_profile_input],\n",
        "          outputs=[count_out, profile_out])\n",
        "        \n",
        "        \n",
        "        model.compile(keras.optimizers.Adam(lr=self.lr),\n",
        "                      loss=['mse', MultichannelMultinomialNLL(2)],\n",
        "                      loss_weights=[self.c_task_weight, self.p_task_weight]) \n",
        "       \n",
        "        return model  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8h_XOldc_3M"
      },
      "source": [
        "PARAMETERS = {\n",
        "    'dataset': 'SOX2',\n",
        "    'input_seq_len': 1346, \n",
        "    'c_task_weight': 0,\n",
        "    'p_task_weight': 1,\n",
        "    'filters': 64, \n",
        "    'n_dil_layers': 6, \n",
        "    'conv1_kernel_size': 21, \n",
        "    'dil_kernel_size': 3, \n",
        "    'outconv_kernel_size': 75, \n",
        "    'optimizer': 'Adam',\n",
        "    'weight_decay': 0.01,\n",
        "    'lr': 0.001, \n",
        "    'kernel_initializer': \"glorot_uniform\",\n",
        "    'seed': 1535\n",
        "}\n",
        "\n",
        "seq_len = 1346\n",
        "out_pred_len = 1000\n",
        "epochs_to_train_for = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhADScgNpYdi"
      },
      "source": [
        "inputs_coordstovals, targets_coordstovals = get_inputs_and_targets(dataset = PARAMETERS['dataset'], seq_len = seq_len, out_pred_len = out_pred_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 586
        },
        "id": "8EDwFxNPdc2d",
        "outputId": "eaa87499-d5cb-4532-af8b-5c96eb7c0ef7"
      },
      "source": [
        "np.random.seed(1234)\n",
        "tf.set_random_seed(1234)\n",
        "\n",
        "standard_model = StandardBPNetArch(is_add = True, **PARAMETERS).get_keras_model()\n",
        "aug_model = StandardBPNetArch(is_add = True, **PARAMETERS).get_keras_model()\n",
        "rcps_model = RCBPNetArch(is_add = True, **PARAMETERS).get_keras_model()\n",
        "cj_trained_model = CJTrainedBPNetArch(is_add = True, **PARAMETERS).get_keras_model()\n",
        "cjrc_model = CJRCWrapperArch(**PARAMETERS).get_keras_model()\n",
        "\n",
        "train_model(PARAMETERS = PARAMETERS, \n",
        "            inputs_coordstovals = inputs_coordstovals, \n",
        "            targets_coordstovals = targets_coordstovals, \n",
        "            epochs_to_train_for = epochs_to_train_for, \n",
        "            model = standard_model, \n",
        "            model_arch = 'Standard-NoRCAug')\n",
        "\n",
        "train_model(PARAMETERS = PARAMETERS, \n",
        "            inputs_coordstovals = inputs_coordstovals, \n",
        "            targets_coordstovals = targets_coordstovals, \n",
        "            epochs_to_train_for = epochs_to_train_for, \n",
        "            model = aug_model, \n",
        "            model_arch = 'Standard-RCAug')\n",
        "\n",
        "train_model(PARAMETERS = PARAMETERS, \n",
        "            inputs_coordstovals = inputs_coordstovals, \n",
        "            targets_coordstovals = targets_coordstovals, \n",
        "            epochs_to_train_for = epochs_to_train_for, \n",
        "            model = rcps_model, \n",
        "            model_arch = 'RCPS')\n",
        "\n",
        "train_model(PARAMETERS = PARAMETERS, \n",
        "            inputs_coordstovals = inputs_coordstovals, \n",
        "            targets_coordstovals = targets_coordstovals, \n",
        "            epochs_to_train_for = epochs_to_train_for, \n",
        "            model = cj_trained_model, \n",
        "            model_arch = 'CJ-trained')\n",
        "\n",
        "train_model(PARAMETERS = PARAMETERS, \n",
        "            inputs_coordstovals = inputs_coordstovals, \n",
        "            targets_coordstovals = targets_coordstovals, \n",
        "            epochs_to_train_for = epochs_to_train_for, \n",
        "            model = cjrc_model, \n",
        "            model_arch = 'CJRCWrapper')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Heads up: coordinates in bed file are assumed to be on the positive strand; if strand in the bed file is improtant to you, please add that feature to SimpleCoordsBatchProducer\n",
            "Heads up: coordinates in bed file are assumed to be on the positive strand; if strand in the bed file is improtant to you, please add that feature to SimpleCoordsBatchProducer\n",
            "Epoch 1/2\n",
            "106/106 [==============================] - 708s 7s/step - loss: 235.0124 - CHIPNexus.SOX2.logcount_loss: 12.6296 - CHIPNexus.SOX2.profile_loss: 235.0124 - val_loss: 210.4256 - val_CHIPNexus.SOX2.logcount_loss: 10.1399 - val_CHIPNexus.SOX2.profile_loss: 210.4256\n",
            "Epoch 2/2\n",
            "  6/106 [>.............................] - ETA: 9:55 - loss: 156.1682 - CHIPNexus.SOX2.logcount_loss: 9.2347 - CHIPNexus.SOX2.profile_loss: 156.1682 "
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-15507309eff0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mepochs_to_train_for\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs_to_train_for\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcj_trained_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             model_arch = 'CJ-trained')\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m train_model(PARAMETERS = PARAMETERS, \n",
            "\u001b[0;32m<ipython-input-7-e881d72e9a5e>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(PARAMETERS, inputs_coordstovals, targets_coordstovals, epochs_to_train_for, model, model_arch)\u001b[0m\n\u001b[1;32m    134\u001b[0m                         \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs_to_train_for\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_batch_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m                         callbacks=[early_stopping_callback, model_history])\n\u001b[0m\u001b[1;32m    137\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mearly_stopping_callback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0msave_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPARAMETERS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_arch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_history\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9gxyLSkfVwj"
      },
      "source": [
        "#Implement CJ-posthoc model \n",
        "from keras.utils import CustomObjectScope\n",
        "from keras.models import load_model\n",
        "\n",
        "with CustomObjectScope({'MultichannelMultinomialNLL': MultichannelMultinomialNLL, \n",
        "                        'RevCompConv1D':RevCompConv1D}):\n",
        "  loaded_model = load_model('Standard-RCAug.h5')\n",
        "\n",
        "  #Let's create the model\n",
        "  #define the inputs\n",
        "  fwd_sequence_input = keras.models.Input(shape=(1346,4))\n",
        "  fwd_patchcap_logcount = keras.models.Input(shape=(1,))\n",
        "  fwd_patchcap_profile = keras.models.Input(shape=(1000,2))\n",
        "  #revcomp sequence input\n",
        "  rev_sequence_input = keras.layers.Lambda(lambda x: x[:,::-1,::-1])(fwd_sequence_input)\n",
        "  rev_patchcap_logcount = keras.layers.Lambda(lambda x: x[:,::-1])(fwd_patchcap_logcount)\n",
        "  #note that last axis is NOT fwd vs reverse strand, but different smoothing levels\n",
        "  # that's why we flip only the middle axis\n",
        "  rev_patchcap_profile = keras.layers.Lambda(lambda x: x[:,::-1])(fwd_patchcap_profile)\n",
        "  #Run the model on the original fwd inputs\n",
        "  fwd_logcount, fwd_profile = loaded_model(\n",
        "      [fwd_sequence_input, fwd_patchcap_logcount, fwd_patchcap_profile])\n",
        "  #Run the original model on the reverse inputs\n",
        "  rev_logcount, rev_profile = loaded_model(\n",
        "      [rev_sequence_input, rev_patchcap_logcount, rev_patchcap_profile])\n",
        "  #Reverse complement rev_logcount and rev_profile to be compatible with fwd\n",
        "  revcompd_rev_logcount = keras.layers.Lambda(lambda x: x[:,::-1])(rev_logcount)\n",
        "  revcompd_rev_profile = keras.layers.Lambda(lambda x: x[:,::-1,::-1])(rev_profile)\n",
        "  #Average the two\n",
        "  avg_logcount = keras.layers.Average()([fwd_logcount, revcompd_rev_logcount])\n",
        "  avg_profile = keras.layers.Average()([fwd_profile, revcompd_rev_profile])\n",
        "  #Create a model that goes from the inputs to the averaged output\n",
        "  cj_posthoc_model = keras.models.Model(inputs=[fwd_sequence_input,\n",
        "                                            fwd_patchcap_logcount,\n",
        "                                            fwd_patchcap_profile],\n",
        "                                        outputs=[avg_logcount, avg_profile])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1FPauvI4cf2",
        "outputId": "d523959e-6334-4b73-b460-aa03f0cc2f9d"
      },
      "source": [
        "cj_posthoc_model.layers"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<keras.engine.input_layer.InputLayer at 0x7fc478f07668>,\n",
              " <keras.engine.input_layer.InputLayer at 0x7fc478f5dfd0>,\n",
              " <keras.engine.input_layer.InputLayer at 0x7fc478efd748>,\n",
              " <keras.layers.core.Lambda at 0x7fc478efdda0>,\n",
              " <keras.layers.core.Lambda at 0x7fc478efdd68>,\n",
              " <keras.layers.core.Lambda at 0x7fc464c30d30>,\n",
              " <keras.engine.training.Model at 0x7fc460b97e10>,\n",
              " <keras.layers.core.Lambda at 0x7fc464c30940>,\n",
              " <keras.layers.core.Lambda at 0x7fc460b97be0>,\n",
              " <keras.layers.merge.Average at 0x7fc478b6cef0>,\n",
              " <keras.layers.merge.Average at 0x7fc478a45da0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWSDtgqdohhR"
      },
      "source": [
        "final_results = {\n",
        "    'jsd': {\n",
        "        'Standard-NoRCAug': [],\n",
        "        'Standard-RCAug': [], \n",
        "        'RCPS': [],\n",
        "        'CJ-trained': [],\n",
        "        'CJ-posthoc': [],\n",
        "        'RCPS-half': [],\n",
        "        \"CJRCWrapper\": []\n",
        "    }, \n",
        "    'pears': {\n",
        "        'Standard-NoRCAug': [],\n",
        "        'Standard-RCAug': [], \n",
        "        'RCPS': [],\n",
        "        'CJ-trained': [],\n",
        "        'CJ-posthoc': [],\n",
        "        'RCPS-half': [],\n",
        "        \"CJRCWrapper\": []\n",
        "    }, \n",
        "    'spear': {\n",
        "        'Standard-NoRCAug': [],\n",
        "        'Standard-RCAug': [], \n",
        "        'RCPS': [],\n",
        "        'CJ-trained': [],\n",
        "        'CJ-posthoc': [],\n",
        "        'RCPS-half': [],\n",
        "        \"CJRCWrapper\": []\n",
        "    }, \n",
        "    'mse': {\n",
        "        'Standard-NoRCAug': [],\n",
        "        'Standard-RCAug': [], \n",
        "        'RCPS': [],\n",
        "        'CJ-trained': [],\n",
        "        'CJ-posthoc': [],\n",
        "        'RCPS-half': [],\n",
        "        \"CJRCWrapper\": []\n",
        "    }\n",
        "}"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuWntqls5kMT"
      },
      "source": [
        "!git clone https://github.com/amtseng/fourier_attribution_priors/\n",
        "!pip install sacred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lyVJyqIhbet",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33a98c73-f465-4b6e-bf1d-5fbd5ad34a88"
      },
      "source": [
        "#Evaluate models: Pearson Correlation, Spearman Correlation, and Jensen-Shannon Divergence\n",
        "#Example for Standard-NoRCAug, similar process can be applied to other models\n",
        "from scipy.special import softmax\n",
        "from fourier_attribution_priors.src.model import profile_performance\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "batch_generator, keras_rc_test_batch_generator = get_test_generator(\n",
        "                    PARAMETERS = PARAMETERS, inputs_coordstovals = inputs_coordstovals, targets_coordstovals = targets_coordstovals)\n",
        "\n",
        "with CustomObjectScope({'MultichannelMultinomialNLL': MultichannelMultinomialNLL, \n",
        "                        'RevCompConv1D':RevCompConv1D}):\n",
        "  loaded_model = load_model('Standard-NoRCAug.h5')\n",
        "\n",
        "preds_profile = []\n",
        "labels_profile = []\n",
        "\n",
        "for batch_idx in range(len(batch_generator)):\n",
        "  batch_inputs, batch_labels = batch_generator[batch_idx]\n",
        "  test_preds = loaded_model.predict(batch_inputs)\n",
        "  preds_profile.extend(softmax(test_preds[1], axis = 1))\n",
        "  labels_profile.extend(batch_labels['CHIPNexus.%s.profile' % PARAMETERS['dataset']])\n",
        "preds_profile = np.array(preds_profile)[:,None,:,:]\n",
        "labels_profile = np.array(labels_profile)[:,None,:,:]\n",
        "\n",
        "jsd =  profile_performance.profile_jsd(true_prof_probs = labels_profile, \n",
        "                          pred_prof_probs = preds_profile, \n",
        "                          jsd_smooth_kernel_sigma = 3)\n",
        "pears, spear, mse = profile_performance.binned_profile_corr_mse( \n",
        "  true_prof_probs = labels_profile, \n",
        "  pred_prof_probs = preds_profile,\n",
        "  prof_count_corr_bin_sizes = [1,5,10],\n",
        "  batch_size=50000\n",
        ")\n",
        "\n",
        "curr_results = {\n",
        "  'jsd': np.average(jsd),\n",
        "  'pears': np.average(pears),\n",
        "  'spear': np.average(spear),\n",
        "  'mse': np.average(mse)\n",
        "}\n",
        "\n",
        "for metrics in curr_results:\n",
        "  final_results[metrics]['Standard-NoRCAug'].append(curr_results[metrics])"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Heads up: coordinates in bed file are assumed to be on the positive strand; if strand in the bed file is improtant to you, please add that feature to SimpleCoordsBatchProducer\n",
            "Heads up: coordinates in bed file are assumed to be on the positive strand; if strand in the bed file is improtant to you, please add that feature to SimpleCoordsBatchProducer\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeOe-k-L6LpW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}